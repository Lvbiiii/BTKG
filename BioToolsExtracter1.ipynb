{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cd1867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biotoolsJSONParser1 import *\n",
    "import json\n",
    "import os\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeeb740",
   "metadata": {},
   "source": [
    "下载BioTools.json数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcbc860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='D:/wenjiannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn/_school/source/jupyter/review/dataset/'\n",
    "url='https://bio.tools/api/t/?format=json&'\n",
    "downloadBiotoolsJson(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba81a6f",
   "metadata": {},
   "source": [
    "提取各个biotoolsID工具的publication下的doi pmid pmcid 和metadata下的abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab007d3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "没有关联论文的工具数量： 2137\n",
      "有关联论文，但是没有摘要的pmid数量： 1507\n",
      "有关联论文，但是没有摘要和pmid的doi数量： 3751\n"
     ]
    }
   ],
   "source": [
    "path='D:\\\\wenjiannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\\\\_school\\\\dataset\\\\biotoolsjson'\n",
    "toolLists=[]\n",
    "Biotools2DOI=dict()\n",
    "Biotools2PMID=dict()\n",
    "Biotools2PMCID=dict()\n",
    "abstractInfo=dict()\n",
    "pmidInfo=dict() # 用于存放BioTools的url上发表过的，有pmid但是没有abtract的pmid,后面通过entraz查找\n",
    "doiInfo=dict() # 用于存放BioTools的url上发表过的，没有abtract没有pmid只能通过doi查找的，后面通过web of science查找\n",
    "nullPublicationCount=0 # 没有发表publication的biotoolsId数量\n",
    "start_page=1\n",
    "max_page=2841\n",
    "# max_page=getMaxPage()\n",
    "for p in range(start_page,max_page+1): \n",
    "    filename=path+'\\\\BioToolsJSON'+str(p)+'.json'\n",
    "    datas=readJSON(filename)\n",
    "    data=datas['list']\n",
    "    for d in data:\n",
    "        biotoolsId=getSimpleInfo(d,'biotoolsID')\n",
    "        pub=getSimpleInfo(d,'publication')\n",
    "        if(len(pub)>0):\n",
    "            Biotools2DOI[biotoolsId]=pub[0]['doi']\n",
    "            Biotools2PMID[biotoolsId]=pub[0]['pmid']\n",
    "            Biotools2PMCID[biotoolsId]=pub[0]['pmcid']\n",
    "            if(pub[0]['metadata'] is not None):\n",
    "                if pub[0]['metadata']['abstract'] is not None :\n",
    "                    abstract=pub[0]['metadata']['abstract']\n",
    "                    abstractInfo[biotoolsId]=abstract\n",
    "                else:\n",
    "                    if pub[0]['pmid'] is not None :\n",
    "                        pmidInfo[biotoolsId]=pub[0]['pmid']\n",
    "                    else:\n",
    "                        doiInfo[biotoolsId]=pub[0]['doi']\n",
    "            else:\n",
    "                if pub[0]['pmid'] is not None :\n",
    "                    pmidInfo[biotoolsId]=pub[0]['pmid']\n",
    "                else:\n",
    "                    doiInfo[biotoolsId]=pub[0]['doi']\n",
    "        else:\n",
    "            nullPublicationCount=nullPublicationCount+1\n",
    "\n",
    "print(\"没有关联论文的工具数量：\",nullPublicationCount)\n",
    "print(\"有关联论文，但是没有摘要的pmid数量：\",len(pmidInfo))\n",
    "print(\"有关联论文，但是没有摘要和pmid的doi数量：\",len(doiInfo))\n",
    "saveNpy('D:\\\\wenjiannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\\\\_school\\\\dataset\\\\output\\\\Biotools2DOI.npy',Biotools2DOI)\n",
    "saveNpy('D:\\\\wenjiannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\\\\_school\\\\dataset\\\\output\\\\Biotools2PMID.npy',Biotools2PMID)\n",
    "saveNpy('D:\\\\wenjiannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\\\\_school\\\\dataset\\\\output\\\\Biotools2PMCID.npy',Biotools2PMCID)\n",
    "saveNpy('D:\\\\wenjiannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\\\\_school\\\\dataset\\\\output\\\\biotolsID2abstractDict.npy',abstractInfo)\n",
    "saveNpy('D:\\\\wenjiannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\\\\_school\\\\dataset\\\\output\\\\biotolsID2pmidDict.npy',pmidInfo)\n",
    "saveNpy('D:\\\\wenjiannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\\\\_school\\\\dataset\\\\output\\\\biotolsID2doiDict.npy',doiInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c282d39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Bio in c:\\users\\bby\\appdata\\roaming\\python\\python311\\site-packages (1.5.9)\n",
      "Requirement already satisfied: biopython>=1.80 in c:\\users\\bby\\appdata\\roaming\\python\\python311\\site-packages (from Bio) (1.81)\n",
      "Requirement already satisfied: requests in d:\\a_software\\anaconda\\lib\\site-packages (from Bio) (2.29.0)\n",
      "Requirement already satisfied: tqdm in d:\\a_software\\anaconda\\lib\\site-packages (from Bio) (4.65.0)\n",
      "Requirement already satisfied: mygene in c:\\users\\bby\\appdata\\roaming\\python\\python311\\site-packages (from Bio) (3.2.2)\n",
      "Requirement already satisfied: pandas in d:\\a_software\\anaconda\\lib\\site-packages (from Bio) (1.5.3)\n",
      "Requirement already satisfied: pooch in d:\\a_software\\anaconda\\lib\\site-packages (from Bio) (1.4.0)\n",
      "Requirement already satisfied: gprofiler-official in c:\\users\\bby\\appdata\\roaming\\python\\python311\\site-packages (from Bio) (1.0.0)\n",
      "Requirement already satisfied: numpy in d:\\a_software\\anaconda\\lib\\site-packages (from biopython>=1.80->Bio) (1.24.3)\n",
      "Requirement already satisfied: biothings-client>=0.2.6 in c:\\users\\bby\\appdata\\roaming\\python\\python311\\site-packages (from mygene->Bio) (0.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\a_software\\anaconda\\lib\\site-packages (from pandas->Bio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\a_software\\anaconda\\lib\\site-packages (from pandas->Bio) (2022.7)\n",
      "Requirement already satisfied: packaging in d:\\a_software\\anaconda\\lib\\site-packages (from pooch->Bio) (23.0)\n",
      "Requirement already satisfied: appdirs in d:\\a_software\\anaconda\\lib\\site-packages (from pooch->Bio) (1.4.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\a_software\\anaconda\\lib\\site-packages (from requests->Bio) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\a_software\\anaconda\\lib\\site-packages (from requests->Bio) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\a_software\\anaconda\\lib\\site-packages (from requests->Bio) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\a_software\\anaconda\\lib\\site-packages (from requests->Bio) (2023.5.7)\n",
      "Requirement already satisfied: colorama in d:\\a_software\\anaconda\\lib\\site-packages (from tqdm->Bio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in d:\\a_software\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->Bio) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Bio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297deb3a",
   "metadata": {},
   "source": [
    "根据文献pmid去pubmed获取摘要，通过Entrez获取。数据在biotolsID2pmidDict.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0baafb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from utils import loadNpy\n",
    "import re\n",
    "from operator import length_hint\n",
    "\n",
    "Entrez.email = '1481995200@qq.com'\n",
    "pmidInfo=loadNpy('D:\\\\wenjiannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\\\\_school\\\\dataset\\\\output\\\\biotolsID2pmidDict.npy').item()\n",
    "abstractInfo=dict()\n",
    "NullFindID=[]\n",
    "# print(pmidInfo) <biotoolsId,pmid>\n",
    "for id in pmidInfo:\n",
    "    pmid=pmidInfo[id]\n",
    "    try:\n",
    "        handle = Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"abstract\")\n",
    "        record = Entrez.read(handle)\n",
    "        # print(record)\n",
    "        if record['PubmedArticle']:\n",
    "            if 'Abstract' in record['PubmedArticle'][0]['MedlineCitation']['Article']:\n",
    "                abstract_list=record['PubmedArticle'][0]['MedlineCitation']['Article']['Abstract']['AbstractText']\n",
    "                abstract=abstract_list[0].__repr__()\n",
    "                length=length_hint(abstract_list)\n",
    "                if length>1:\n",
    "                    abstract='; '.join(abstract_list)\n",
    "            else:\n",
    "                abstract=' '\n",
    "        else:\n",
    "            abstract=' '\n",
    "        abstractInfo[id]=abstract\n",
    "        # print(abstract)\n",
    "        handle.close()\n",
    "    except Exception as e:\n",
    "        print(id)\n",
    "        NullFindID.append(id)\n",
    "        \n",
    "#要将新获得的论文摘要存入之前的文件吗？\n",
    "print(len(pmidInfo))\n",
    "saveNpy('D:\\\\wenjiannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\\\\_school\\\\dataset\\\\output\\\\biotolsID2abstractDict1.npy',abstractInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4588ab1",
   "metadata": {},
   "source": [
    "其余的论文摘要通过doi在谷歌学术上获取，数据在biotolsID2doiDict.npy，在DownLoadGoogleScholarAbstractByDOI.py运行,未实现代理ip需隔段时间手动修改爬取范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e80bbc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=10.1101%2F2023.08.28.555103&btnG=\n",
      "['N', 'o', 'n', '-', 'c', 'o', 'd', 'i', 'n', 'g', ' ', 'R', 'N', 'A', 's', ' ', 'p', 'l', 'a', 'y', ' ', 'a', ' ', 'g', 'r', 'e', 'a', 't', ' ', 'v', 'a', 'r', 'i', 'e', 't', 'y', ' ', 'o', 'f', ' ', 'r', 'o', 'l', 'e', 's', ' ', 'i', 'n', ' ', 'm', 'a', 'n', 'y', ' ', 'c', 'e', 'l', 'l', 'u', 'l', 'a', 'r', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', 'e', 's', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', 'i', 'r', ' ', 's', 'p', 'a', 't', 'i', 'a', 'l', ' ', 's', 't', 'r', 'u', 'c', 't', 'u', 'r', 'e', ' ', 'k', 'n', 'o', 'w', 'n', ' ', 't', 'o', ' ', 'd', 'i', 'c', 't', 'a', 't', 'e', ' ', 't', 'h', 'e', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', 'i', 'n', 'g', '.', ' ', 'R', 'N', 'A', ' ', 's', 'e', 'c', 'o', 'n', 'd', 'a', 'r', 'y', ' ', 's', 't', 'r', 'u', 'c', 't', 'u', 'r', 'e', ' ', 'l', 'a', 'r', 'g', 'e', 'l', 'y', ' ', 'd', 'e', 't', 'e', 'r', 'm', 'i', 'n', 'e', 's', ' ', 't', 'h', 'e', ' ', 'm', 'o', 'l', 'e', 'c', 'u', 'l', 'e', \"'\", 's', ' ', 'g', 'l', 'o', 'b', 'a', 'l', ' ', 'f', 'o', 'l', 'd', ',', ' ', 'w', 'h', 'i', 'c', 'h', ' ', 't', 'o', 'g', 'e', 't', 'h', 'e', 'r', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'p', 'a', 'u', 'c', 'i', 't', 'y', ' ', 'o', 'f', ' ', 'e', 'x', 'p', 'e', 'r', 'i', 'm', 'e', 'n', 't', 'a', 'l', 'l', 'y', ' ', 'd', 'e', 't', 'e', 'r', 'm', 'i', 'n', 'e', 'd', ' ', 'R', 'N', 'A', ' ', '3', 'D', ' ', 's', 't', 'r', 'u', 'c', 't', 'u', 'r', 'e', 's', ' ', 'm', 'a', 'k', 'e', 's', ' ', 'i', 't', 's', ' ', 'k', 'n', 'o', 'w', 'l', 'e', 'd', 'g', 'e', ' ', 'c', 'r', 'u', 'c', 'i', 'a', 'l', ' ', 'f', 'o', 'r', ' ', 'd', 'e', 't', 'e', 'r', 'm', 'i', 'n', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'm', 'o', 'l', 'e', 'c', 'u', 'l', 'e', '.', ' ', 'C', 'u', 'r', 'r', 'e', 'n', 't', 'l', 'y', ',', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 'n', 'o', ' ', 'o', 'n', 'e', ' ', 'g', 'o', 'o', 'd', ' ', 's', 'o', 'l', 'u', 't', 'i', 'o', 'n', ' ', 'f', 'o', 'r', ' ', 'd', 'e', ' ', 'n', 'o', 'v', 'o', ' ', 'R', 'N', 'A', ' ', 's', 'e', 'c', 'o', 'n', 'd', 'a', 'r', 'y', ' ', 's', 't', 'r', 'u', 'c', 't', 'u', 'r', 'e', ' ', 'p', 'r', 'e', 'd', 'i', 'c', 't', 'i', 'o', 'n', ',', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'e', 'x', 'i', 's', 't', 'i', 'n', 'g', ' ', 'm', 'e', 't', 'h', 'o', 'd', 's', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'm', 'o', 'r', 'e', ' ', 'a', 'n', 'd', ' ', 'm', 'o', 'r', 'e', ' ', 'c', 'o', 'm', 'p', 'l', 'i', 'c', 'a', 't', 'e', 'd', ' ', 'w', 'i', 't', 'h', 'o', 'u', 't', ' ', 's', 'u', 'b', 's', 't', 'a', 'n', 't', 'i', 'a', 'l', ' ', 'p', 'r', 'o', 'g', 'r', 'e', 's', 's', ' ', 'i', 'n', '\\xa0', '…']\n",
      "https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=10.1093%2Fnar&btnG=\n",
      "error, retrying ... \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mD:\\wenjiannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\\_school\\source\\jupyter\\review\\DownLoadGoogleScholarAbstractByDOI.py:58\u001b[0m, in \u001b[0;36mget_paper_list_by_doi\u001b[1;34m(doi, myrow, mycolumn, max_capacity, debug_mode, save_file, retry_times)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     data\u001b[38;5;241m.\u001b[39mextend(get_paper_page(url))\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mD:\\wenjiannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\\_school\\source\\jupyter\\review\\DownLoadGoogleScholarAbstractByDOI.py:27\u001b[0m, in \u001b[0;36mget_paper_page\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     24\u001b[0m soup = BeautifulSoup(html)\n\u001b[0;32m     25\u001b[0m # data = [[div.select('.gs_rt > a')[0].text, div.select('.gs_fl > a')[2].string, re.search(\"- .*?\\</div>\", str(div.select('.gs_a')[0])).group()[1:-6].replace(\"\\xa0\", \"\"), div.select('.gs_rt > a')[0][\"href\"]] for div in soup.select('.gs_ri')]\n\u001b[0;32m     26\u001b[0m # data = [[x[0], int(x[1][6:]) if x[1] != None and x[1].startswith(\"被引用次数\") else 0, x[2], x[3]] for x in data]\n\u001b[1;32m---> 27\u001b[0m # for div in soup.select('gs_rs gs_fma_s'):\n\u001b[0;32m     28\u001b[0m data = soup.find('div',attrs={'class':'gs_rs gs_fma_s'}).text\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m Tablerow \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      6\u001b[0m     doi \u001b[38;5;241m=\u001b[39m readExcel(InPutPath, Tablerow, TableColumn)\n\u001b[1;32m----> 7\u001b[0m     get_paper_list_by_doi(doi, Tablerow, TableColumn, max_capacity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, debug_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, save_file\u001b[38;5;241m=\u001b[39mOutputPath)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\wenjiannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\\_school\\source\\jupyter\\review\\DownLoadGoogleScholarAbstractByDOI.py:67\u001b[0m, in \u001b[0;36mget_paper_list_by_doi\u001b[1;34m(doi, myrow, mycolumn, max_capacity, debug_mode, save_file, retry_times)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m debug_mode:\n\u001b[0;32m     66\u001b[0m             traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m---> 67\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m     68\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# data: [论文标题, 引用数, 发表时间及机构缩写, 论文链接]\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from DownLoadGoogleScholarAbstractByDOI import *\n",
    "InPutPath = r'D:\\\\wenjiannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\\\\_school\\\\dataset\\\\biotolsID2doiDict.xlsx'\n",
    "OutputPath = r'D:\\\\wenjiannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\\\\_school\\\\dataset\\\\abstract.xlsx'\n",
    "TableColumn = 2\n",
    "for Tablerow in range(1,10):\n",
    "    doi = readExcel(InPutPath, Tablerow, TableColumn)\n",
    "    get_paper_list_by_doi(doi, Tablerow, TableColumn, max_capacity=100, debug_mode=False, save_file=OutputPath)\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f12d85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
